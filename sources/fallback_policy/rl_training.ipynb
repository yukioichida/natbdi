{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-18T19:13:38.753921Z",
     "start_time": "2024-08-18T19:13:35.923413Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sources.cl_nli.model import SimCSE\n",
    "from sources.fallback_policy.model import QNetwork\n",
    "from sources.scienceworld import parse_beliefs\n",
    "\n",
    "from sources.fallback_policy.replay import ReplayMemory, Transition"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Reading goldpath trajectories",
   "id": "43b359c162afc26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:13:39.245452Z",
     "start_time": "2024-08-18T19:13:39.239143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "goldpath_file = \"/opt/data/scienceworld-goldpaths/goldsequences-0.json\"\n",
    "\n",
    "with open(goldpath_file) as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "json_data = json_data['0']\n",
    "\n",
    "json_data.keys()"
   ],
   "id": "665684dbb9595f29",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['taskIdx', 'taskName', 'goldActionSequences'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:13:47.366720Z",
     "start_time": "2024-08-18T19:13:39.989251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ckpt = \"/opt/models/simcse_default/version_0/v0-epoch=4-step=18304-val_nli_loss=0.658-train_loss=0.551.ckpt\"\n",
    "\n",
    "model: SimCSE = SimCSE.load_from_checkpoint(ckpt).eval()\n",
    "hf_model_name = model.hparams['hf_model_name']\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_name)"
   ],
   "id": "9e98b425dbabce41",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ichida/miniconda3/envs/default_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create memory buffer",
   "id": "b8bd533771827901"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:13:49.105567Z",
     "start_time": "2024-08-18T19:13:49.101983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode(texts: list[str], max_size: int = 25, include_cls: bool = True) -> torch.Tensor:\n",
    "    if include_cls:\n",
    "        cls_token = tokenizer.cls_token\n",
    "        texts = [cls_token] + texts\n",
    "    pad_size = max_size - len(texts)\n",
    "    padding = [tokenizer.pad_token for _ in range(pad_size)]\n",
    "    texts = texts + padding\n",
    "    tokenized_text = tokenizer(texts, padding='longest', truncation=True,\n",
    "                               return_tensors='pt').to(model.device)\n",
    "    embeddings = model.encode(tokenized_text).detach().unsqueeze(0)  # batch axis\n",
    "    return embeddings\n",
    "\n"
   ],
   "id": "7c004c5d2c3bf248",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:15:27.187623Z",
     "start_time": "2024-08-18T19:15:23.638130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gold_sequence = json_data['goldActionSequences'][0]['path']\n",
    "\n",
    "goal = json_data['goldActionSequences'][0]['taskDescription'].split('.')[0]\n",
    "variation_idx = json_data['goldActionSequences'][0]['variationIdx']\n",
    "print(f\"Goal: {goal} - variation {variation_idx}\")\n",
    "\n",
    "all_lens = []\n",
    "\n",
    "trajectories_bdi = []\n",
    "\n",
    "memory_buffer = ReplayMemory(1000)\n",
    "\n",
    "last_reward = 0\n",
    "for i, trajectory in enumerate(gold_sequence):\n",
    "    observation = trajectory['observation']\n",
    "    look_around = trajectory['freelook']\n",
    "    inventory = trajectory['inventory']\n",
    "    belief_base = parse_beliefs(observation=observation, look=look_around, inventory=inventory)\n",
    "\n",
    "    next_trajectory = gold_sequence[i + 1]\n",
    "    next_belief_base = parse_beliefs(observation=next_trajectory['observation'],\n",
    "                                     look=next_trajectory['freelook'],\n",
    "                                     inventory=next_trajectory['inventory'])\n",
    "\n",
    "    reward = float(trajectory['score']) - last_reward\n",
    "    last_reward = float(trajectory['score'])\n",
    "    is_done = trajectory['isCompleted']\n",
    "    if is_done == 'true':\n",
    "        next_state = \"\"\n",
    "        print(\"finish\")\n",
    "        break\n",
    "        # ou break de repente aqui\n",
    "\n",
    "    memory_buffer.push(\n",
    "            Transition(\n",
    "                    belief_base=encode(belief_base + [goal]),\n",
    "                    num_beliefs=len(belief_base) + 1,  # including goal\n",
    "                    action=encode([trajectory['action']], max_size=1, include_cls=False),\n",
    "                    next_belief_base=encode(next_belief_base + [goal]),\n",
    "                    num_next_beliefs=len(next_belief_base) + 1,\n",
    "                    next_action=encode([next_trajectory['action']], max_size=1, include_cls=False),\n",
    "                    reward=reward,\n",
    "                    done=True if is_done == 'true' else False\n",
    "            )\n",
    "    )\n",
    "\n",
    "    #print(f\"Step {i} - reward: {reward:.3f} - is_done: {is_done}  - action: {trajectory['action']}\")"
   ],
   "id": "585ca7affeca7d46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal: Your task is to boil water - variation 0\n",
      "finish\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:15:29.279120Z",
     "start_time": "2024-08-18T19:15:29.275080Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "77230e26d7a2d6a3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 'use thermometer in inventory on substance in metal pot',\n",
       " 'observation': 'the thermometer measures a temperature of 98 degrees celsius',\n",
       " 'score': '1.0',\n",
       " 'isCompleted': 'true',\n",
       " 'freelook': 'This room is called the kitchen. In it, you see: \\n\\ta substance called soap\\n\\ta painting\\n\\ta counter. On the counter is: a bowl (containing a banana, a potato, a red apple, an orange), a drawer.\\n\\ta sink, which is turned off. In the sink is: nothing.\\n\\ta table. On the table is: a glass cup (containing nothing).\\n\\ta chair. On the chair is: nothing.\\n\\ta freezer. The freezer door is closed. \\n\\ta lighter\\n\\ta stopwatch, which is deactivated. \\n\\ta fridge. The fridge door is closed. \\n\\ta substance called air\\n\\ta cupboard. The cupboard door is open. In the cupboard is: a tin cup (containing nothing), a ceramic cup (containing nothing), a drawer.\\n\\ta oven, which is turned off. The oven door is closed. \\n\\ta glass jar (containing a substance called sodium chloride)\\n\\tthe agent\\n\\ta stove, which is turned on. On the stove is: a metal pot (containing a substance called water).\\nYou also see:\\n\\tA door to the outside (that is open)\\n\\tA door to the bathroom (that is open)\\n\\tA door to the hallway (that is open)\\n',\n",
       " 'inventory': 'In your inventory, you see:\\n\\ta thermometer, currently reading a temperature of 10 degrees celsius\\n\\tan orange\\n'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:13:56.627162Z",
     "start_time": "2024-08-18T19:13:56.594587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "with open('memory_buffer.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(memory_buffer, pickle_file)\n"
   ],
   "id": "cef9cb298593f5cb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:13:56.656458Z",
     "start_time": "2024-08-18T19:13:56.628490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('memory_buffer.pkl', 'rb') as pickle_file:\n",
    "    memory_buffer_p = pickle.load(pickle_file)\n"
   ],
   "id": "8da3d0207a90caa5",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train q-network using annotated trajectories",
   "id": "c326011ec0529219"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:14:33.808365Z",
     "start_time": "2024-08-18T19:14:33.788239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sources.fallback_policy.model import SimpleQNetwork\n",
    "\n",
    "#network = QNetwork(768, 768, 768, n_blocks=5)\n",
    "network = SimpleQNetwork(768, 768, 5)\n",
    "network = network.to('cuda')\n",
    "network.train()\n",
    "\n",
    "num_parameters = sum(p.numel() for p in network.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters: {num_parameters}\")\n",
    "network"
   ],
   "id": "19ec2323278a0b35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1180416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleQNetwork(\n",
       "  (hidden): Linear(in_features=1536, out_features=768, bias=False)\n",
       "  (q_value_layer): Linear(in_features=768, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:14:39.667568Z",
     "start_time": "2024-08-18T19:14:39.320644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# q-learning \n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "optimizer = torch.optim.AdamW(network.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(100):\n",
    "    batch = memory_buffer.sample(BATCH_SIZE)\n",
    "    belief_base_emb = torch.cat([b.belief_base for b in batch], dim=0)\n",
    "    num_belief_emb = [b.num_beliefs for b in batch]\n",
    "    next_belief_base_emb = torch.cat([b.next_belief_base for b in batch], dim=0)\n",
    "    num_next_belief_emb = [b.num_next_beliefs for b in batch]\n",
    "    rewards = torch.tensor([(b.reward + 1) for b in batch]).to(model.device)\n",
    "    actions = torch.cat([b.action for b in batch]).squeeze(1)  # removing mid axis [bs, ?, a_dim] -> [bs, a_dim]\n",
    "    next_actions = torch.cat([b.next_action for b in batch]).squeeze(\n",
    "            1)  # removing mid axis [bs, ?, a_dim] -> [bs, a_dim]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # Q(s', a')\n",
    "    next_q_values = network(belief_base=next_belief_base_emb,\n",
    "                            belief_base_sizes=num_next_belief_emb,\n",
    "                            action_tensors=next_actions)\n",
    "    best_next_q_values = next_q_values.squeeze(-1)\n",
    "    targets = rewards + (GAMMA * best_next_q_values)\n",
    "\n",
    "    # Q(s, a)\n",
    "    q_values = network(belief_base=belief_base_emb, belief_base_sizes=num_belief_emb, action_tensors=actions)\n",
    "    loss = F.smooth_l1_loss(q_values.squeeze(-1), targets.detach())\n",
    "    if epoch % 10 == 0:\n",
    "        #print(f\"[{epoch}] q-values {q_values.squeeze(-1)}, targets {targets.squeeze(-1)} - loss {loss.item()}\")\n",
    "        print(f\"[{epoch}] loss {loss.item()}\")\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(network.parameters(), 1.)\n",
    "    optimizer.step()\n"
   ],
   "id": "76073dcf3fff52a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] loss 0.501124918460846\n",
      "[10] loss 0.5170617699623108\n",
      "[20] loss 0.6482571363449097\n",
      "[30] loss 0.4974153935909271\n",
      "[40] loss 0.5479430556297302\n",
      "[50] loss 0.6194901466369629\n",
      "[60] loss 0.6764197945594788\n",
      "[70] loss 0.6734042167663574\n",
      "[80] loss 0.9031770825386047\n",
      "[90] loss 0.7735987901687622\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluating Q-network in scienceworld environment",
   "id": "a72e5f68d324132d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:08:18.760283Z",
     "start_time": "2024-08-18T19:08:18.757667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scienceworld import ScienceWorldEnv\n",
    "\n",
    "network = network.eval()"
   ],
   "id": "8198c6a7a110af9b",
   "outputs": [],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:08:20.219887Z",
     "start_time": "2024-08-18T19:08:19.186106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = ScienceWorldEnv()\n",
    "variation_idx = json_data['goldActionSequences'][0]['variationIdx']\n",
    "env.load(\"boil\", variation_idx)"
   ],
   "id": "b39fe1e94d67835",
   "outputs": [],
   "execution_count": 190
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:08:20.240868Z",
     "start_time": "2024-08-18T19:08:20.235986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def select_action(obs, look, inv, goal, candidate_actions):\n",
    "    belief_base = parse_beliefs(observation=obs, look=look, inventory=inv) + [goal]\n",
    "    encoded_belief_base = encode(belief_base, max_size=len(belief_base) + 1)  # including CLS token\n",
    "    encoded_actions = encode(candidate_actions, max_size=len(candidate_actions), is_action=True)\n",
    "    encoded_actions = encoded_actions.squeeze(0)\n",
    "    num_actions, action_dim = encoded_actions.size()\n",
    "    repeat_encoded_belief_base = encoded_belief_base.repeat(num_actions, 1, 1)\n",
    "    q_values = network(belief_base=repeat_encoded_belief_base,\n",
    "                       belief_base_sizes=[len(belief_base)],\n",
    "                       action_tensors=encoded_actions)\n",
    "    selected_action = q_values.argmax()\n",
    "    return candidate_actions[selected_action.item()]\n"
   ],
   "id": "d7e408742b8c7c77",
   "outputs": [],
   "execution_count": 191
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:08:30.152443Z",
     "start_time": "2024-08-18T19:08:29.869848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    max_steps = 1\n",
    "    action = \"look around\"\n",
    "    for i in range(max_steps):\n",
    "        print(f\"Step {i} - reward: {reward:.3f} - is_done: {is_done} - action: {action}\")\n",
    "        obs, reward, is_done, info = env.step(action)\n",
    "        belief_base = parse_beliefs(observation=obs, look=info['look'], inventory=info['inv']) + [goal]\n",
    "        encoded_belief_base = encode(belief_base, max_size=len(belief_base))\n",
    "        encoded_actions = encode(info['valid'], max_size=len(info['valid']), include_cls=False)\n",
    "        encoded_actions = encoded_actions.squeeze(0)\n",
    "        num_actions, action_dim = encoded_actions.size()\n",
    "        repeat_encoded_belief_base = encoded_belief_base.repeat(num_actions, 1, 1)\n",
    "        q_values = network(belief_base=repeat_encoded_belief_base,\n",
    "                           belief_base_sizes=[len(belief_base)],\n",
    "                           action_tensors=encoded_actions)\n",
    "        selected_action = q_values.argmax()\n",
    "        action = info['valid'][selected_action.item()]\n"
   ],
   "id": "dc06bae544bbd7d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 - reward: 0.000 - is_done: False - action: look around\n"
     ]
    }
   ],
   "execution_count": 193
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:08:34.202422Z",
     "start_time": "2024-08-18T19:08:34.176657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "values, idxs = torch.sort(q_values.squeeze(1), descending=True)\n",
    "for i, idx in enumerate(idxs):\n",
    "    if info['valid'][idx] == 'open door to kitchen':\n",
    "        print(f\"act {info['valid'][idx]} - q_value: {values[i]:.3f}\")"
   ],
   "id": "ccf6e31a9043452a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act open door to kitchen - q_value: -7.182\n"
     ]
    }
   ],
   "execution_count": 194
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:08:38.311490Z",
     "start_time": "2024-08-18T19:08:38.297115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "values, idxs = torch.sort(q_values.squeeze(1), descending=True)\n",
    "for i, idx in enumerate(idxs[:10]):\n",
    "    print(f\"act {info['valid'][idx]} - q_value: {values[i]:.3f}\")"
   ],
   "id": "a76fc3360319af2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act connect agent to air - q_value: 105.128\n",
      "act open door to kitchen - q_value: -7.182\n",
      "act open door to workshop - q_value: -7.202\n",
      "act connect door to kitchen to art studio door - q_value: -7.203\n",
      "act connect door to living room to door to workshop - q_value: -7.206\n",
      "act connect air to door to greenhouse - q_value: -7.208\n",
      "act open art studio door - q_value: -7.209\n",
      "act connect kitchen to door to greenhouse - q_value: -7.212\n",
      "act connect art studio to door to kitchen - q_value: -7.213\n",
      "act look at door to kitchen - q_value: -7.215\n"
     ]
    }
   ],
   "execution_count": 195
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:08:49.876988Z",
     "start_time": "2024-08-18T19:08:49.870521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "turn = 1\n",
    "print(gold_sequence[turn]['action'])\n",
    "annotated_belief_base = parse_beliefs(observation=gold_sequence[turn - 1]['observation'],\n",
    "                                      look=gold_sequence[turn - 1]['freelook'],\n",
    "                                      inventory=gold_sequence[turn - 1]['inventory']) + [goal]\n",
    "#annotated_belief_base = ['This room is called the kitchen', 'You see a anchor'] + [goal]\n",
    "print(len(annotated_belief_base))\n",
    "annotated_belief_base"
   ],
   "id": "293dd8fddb69b724",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open door to kitchen\n",
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This room is called the hallway.',\n",
       " 'You see a picture',\n",
       " 'You see a substance called air',\n",
       " 'You see the agent',\n",
       " 'A door to the green house (that is open)',\n",
       " 'A door to the living room (that is open)',\n",
       " 'A door to the art studio (that is open)',\n",
       " 'A door to the kitchen (that is open)',\n",
       " 'A door to the bedroom (that is open)',\n",
       " 'A door to the workshop (that is open)',\n",
       " 'In your inventory, you see: an orange ',\n",
       " '',\n",
       " 'Your task is to boil water']"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 197
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:10:12.163459Z",
     "start_time": "2024-08-18T19:10:12.117967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded_belief_base = encode(annotated_belief_base, max_size=len(annotated_belief_base) + 1)\n",
    "print(encoded_belief_base.size())\n",
    "#candidate_actions = info['valid']\n",
    "candidate_actions = ['focus on bedroom door', 'open door to kitchen']\n",
    "encoded_actions = encode(candidate_actions, include_cls=False, max_size=len(candidate_actions))\n",
    "encoded_actions = encoded_actions.squeeze(0)\n",
    "print(encoded_actions.size())\n",
    "num_actions, action_dim = encoded_actions.size()\n",
    "repeat_encoded_belief_base = encoded_belief_base.repeat(num_actions, 1, 1)\n",
    "q_values = network(belief_base=repeat_encoded_belief_base,\n",
    "                   belief_base_sizes=[len(annotated_belief_base) + 1 for _ in range(num_actions)],\n",
    "                   action_tensors=encoded_actions)\n",
    "print(q_values.size())"
   ],
   "id": "8efb911a1d409f95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14, 768])\n",
      "torch.Size([2, 768])\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "execution_count": 202
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:10:14.407742Z",
     "start_time": "2024-08-18T19:10:14.402100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "values, idxs = torch.sort(q_values.squeeze(1), descending=True)\n",
    "for i, idx in enumerate(idxs[:5]):\n",
    "    print(f\"act {candidate_actions[idx]} - q_value: {values[i]:.3f}\")"
   ],
   "id": "b693ac864be057a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act open door to kitchen - q_value: 105.626\n",
      "act focus on bedroom door - q_value: 105.429\n"
     ]
    }
   ],
   "execution_count": 203
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:10:21.898603Z",
     "start_time": "2024-08-18T19:10:21.892984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "values, idxs = torch.sort(q_values.squeeze(1), descending=True)\n",
    "for i, idx in enumerate(idxs[:5]):\n",
    "    print(f\"act {candidate_actions[idx]} - q_value: {values[i]:.3f}\")"
   ],
   "id": "8918f7f9334a89a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act open door to kitchen - q_value: 105.626\n",
      "act focus on bedroom door - q_value: 105.429\n"
     ]
    }
   ],
   "execution_count": 204
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:11:01.734600Z",
     "start_time": "2024-08-18T19:11:01.714931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embeddings_a = encode(['you see a door, which is open', 'you see a door, which is closed'], max_size=3)\n",
    "emb_a = embeddings_a[0, 1, :]\n",
    "emb_b = embeddings_a[0, 2, :]\n",
    "\n",
    "enc_a = network.belief_base_encoder(embeddings_a, [3])\n",
    "\n",
    "nn.CosineSimilarity(dim=-1)(emb_a, emb_b)"
   ],
   "id": "8cd299eec3b8a131",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1126, device='cuda:0')"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 211
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:11:02.293329Z",
     "start_time": "2024-08-18T19:11:02.263610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "belief_test = ['you see a door, which is open', 'you see a door, which is not closed']\n",
    "print(len(belief_test))\n",
    "embeddings_b = encode(belief_test, max_size=3)\n",
    "emb_a = embeddings_b[0, 1, :]\n",
    "emb_b = embeddings_b[0, 2, :]\n",
    "\n",
    "enc_b = network.belief_base_encoder(embeddings_b, [3])\n",
    "\n",
    "nn.CosineSimilarity(dim=-1)(emb_a, emb_b)"
   ],
   "id": "66e2b0b444fb75b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9416, device='cuda:0')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 212
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:11:02.774180Z",
     "start_time": "2024-08-18T19:11:02.768266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# HMMM WE HAVE A PROBLEM, POOLED BELIEF REPRESENTATION HAS THE SAME REPRESENTATION FOR EVERY BELIEF COMBINATION, WE NEED TO CHECK THE POOLING METHOD\n",
    "# THIS IS THE PROBLEM OF TRAINING RL\n",
    "nn.CosineSimilarity(dim=-1)(enc_a, enc_b), embeddings_a.size(), embeddings_a.size()"
   ],
   "id": "6e6bee40db9aaf04",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000], device='cuda:0', grad_fn=<SumBackward1>),\n",
       " torch.Size([1, 3, 768]),\n",
       " torch.Size([1, 3, 768]))"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 213
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T18:55:41.503591Z",
     "start_time": "2024-08-18T18:55:41.498753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cls_emb = embeddings_b[:, 0, :]\n",
    "\n",
    "mask = embeddings_b != cls_emb\n",
    "mask\n",
    "\n",
    "mean_embedding_b = ((embeddings_b * mask).sum(dim=1) / mask.sum(dim=1))\n",
    "mean_embedding_b.size()"
   ],
   "id": "db5a2781c7900dff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:11:31.112754Z",
     "start_time": "2024-08-18T19:11:31.088032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embeddings_c = encode(belief_test, max_size=6)\n",
    "embeddings_c.size(), len(belief_test)"
   ],
   "id": "f2bec42008e23055",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 768]), 2)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 215
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:11:46.277345Z",
     "start_time": "2024-08-18T19:11:46.269793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mask = torch.zeros_like(embeddings_c)\n",
    "for size in [len(belief_test)]:\n",
    "    mask[:, :size] = 1\n",
    "\n",
    "mask"
   ],
   "id": "e55d1d3dc9dc0402",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 217
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:12:36.219325Z",
     "start_time": "2024-08-18T19:12:36.215299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_mean = (embeddings_c * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "y_mean.size()"
   ],
   "id": "82219488fba98520",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 220
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:33:55.309386Z",
     "start_time": "2024-08-18T19:33:55.278051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_text = tokenizer(\"take metal pot inside the table\", padding='longest', truncation=True,\n",
    "                               return_tensors='pt').to(model.device)\n",
    "embeddings_a = model.encode(tokenized_text)\n",
    "\n",
    "tokenized_text = tokenizer(\"look at metal pot inside the water\", padding='longest', truncation=True,\n",
    "                               return_tensors='pt').to(model.device)\n",
    "embeddings_b = model.encode(tokenized_text)\n",
    "\n",
    "\n",
    "\n",
    "nn.functional.cosine_similarity(embeddings_a, embeddings_b)"
   ],
   "id": "bc5dbf8844a4a5f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3574], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a2f8df47fa3f3d81"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
