from typing import Dict, Any

import lightning as L
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim.lr_scheduler import CosineAnnealingLR, ExponentialLR
from transformers import PreTrainedModel, PretrainedConfig, get_linear_schedule_with_warmup, AutoModel, AutoConfig, \
    AutoTokenizer


class SimCSE(L.LightningModule):

    def __init__(self,
                 hf_model_name: str,
                 neg_weight: float,
                 training_epochs: int,
                 lr: float = 5e-5,
                 cl_temp: float = 0.05,
                 max_seq_len: int = 32,
                 val_batch_size: int = 128,
                 use_mlp: bool = False):
        super().__init__()
        self.save_hyperparameters("cl_temp",
                                  "training_epochs",
                                  "hf_model_name",
                                  "neg_weight",
                                  "lr",
                                  "max_seq_len",
                                  "val_batch_size",
                                  "use_mlp")
        self.backbone_model = AutoModel.from_pretrained(hf_model_name, add_pooling_layer=use_mlp)
        backbone_config = AutoConfig.from_pretrained(hf_model_name)
        #self.similarity_fn = nn.CosineSimilarity(dim=-1)
        self.tokenizer = AutoTokenizer.from_pretrained(hf_model_name)

    def similarity_fn(self, tensor_a, tensor_b, use_cosine:bool = True):
        if use_cosine:
            sim_function = nn.CosineSimilarity(dim=-1)
            return sim_function(tensor_a, tensor_b)
        else:
            # dot product
            return torch.matmul(tensor_a, tensor_b.T)


    def encode(self, x):  # TODO incluir mlp tambem em validation
        x = self.backbone_model(**x)

        if self.hparams['use_mlp']:
            x = x.pooler_output
        else:
            x = x.last_hidden_state[:, 0]  # CLS token
        return x

    def forward(self, batch):
        preprocess_batch = self._preprocess_batch(batch)
        s1_emb = self.encode(preprocess_batch['s1'])
        s2_emb = self.encode(preprocess_batch['s2'])
        s3_emb = self.encode(preprocess_batch['s3'])
        cl = self.cl_step(s1_emb=s1_emb, s2_emb=s2_emb, s3_emb=s3_emb,
                          nli_weight=preprocess_batch['nli_weight'])
        return cl

    def training_step(self, batch, batch_idx):
        sim_matrix = self.forward(batch=batch)
        batch_size = sim_matrix.size(0)
        labels = torch.arange(batch_size).long().to('cuda')
        loss = F.cross_entropy(sim_matrix, labels)
        #loss = self._custom_cl_loss(sim_matrix)
        self.log("train_loss", loss, prog_bar=True, on_step=True, on_epoch=True)
        return loss


    def _regression_cl_loss(self, sim_matrix):
        batch_size = sim_matrix.size(0)
        pos_class = torch.eye(batch_size).to(self.device)
        neg_class = torch.zeros(batch_size, batch_size).fill_diagonal_(-1).to(self.device)
        label =  torch.cat([pos_class, neg_class],dim=1)
        tanh_sim = torch.tanh(sim_matrix)
        #return F.mse_loss(tanh_sim, label)
        return F.hinge_embedding_loss(tanh_sim, label)

    def _tokenize_batch(self, sentence_batch):
        return self.tokenizer(sentence_batch,
                              truncation=True,
                              max_length=self.hparams['max_seq_len'],
                              padding="longest",
                              return_tensors='pt').to(self.device)

    def _preprocess_batch(self, batch) -> Dict[str, Any]:
        """
        Retrieve each sentence BatchEncoder into a new batch
        :param batch: batch generated by dataset
        :return: dict containing all batches (s1, s2, s3, nli_label)
        """
        s1_batch = self._tokenize_batch(batch['s1'])
        s2_batch = self._tokenize_batch(batch['s2'])
        s3_batch = self._tokenize_batch(batch['s3'])
        return {'s1': s1_batch,
                's2': s2_batch,
                's3': s3_batch,
                'nli_weight': batch['nli_weight'].to(self.device)}

    def validation_step(self, batch, batch_idx):
        loss = self._cl_nli_validation(batch)
        return loss

    def _default_cl_validation(self, batch):
        sim_matrix = self.forward(batch=batch)
        batch_size = sim_matrix.size(0)
        labels = torch.arange(batch_size).long().to('cuda')
        loss = F.cross_entropy(sim_matrix, labels)
        loss = self._custom_cl_loss(sim_matrix)
        self.log("val_cl_loss", loss, prog_bar=True, on_step=True, on_epoch=True)
        return loss

    def _cl_nli_validation(self, batch):
        s1_batch = self._tokenize_batch(batch['s1'])
        s2_batch = self._tokenize_batch(batch['s2'])
        s1_emb = self.encode(s1_batch)
        s2_emb = self.encode(s2_batch)
        e_sim = self.similarity_fn(s1_emb, s2_emb)
        score = (e_sim - batch['nli_objective']).abs().mean()
        self.log("val_nli_loss", score, prog_bar=True, on_step=True, on_epoch=True,
                 batch_size=self.hparams['val_batch_size'])
        return score

    def cl_step(self,
                s1_emb: torch.Tensor,
                s2_emb: torch.Tensor,
                s3_emb: torch.Tensor,
                nli_weight: torch.Tensor):
        """
        Contrastive Learning step
        :param s1_emb: sentence embeddings
        :param s2_emb: entailment sentence embeddings
        :param s3_emb: contradiction sentence embeddings
        :param nli_weight:
        :return:
        """
        # pairs to be aproximated
        similarity_matrix = self.similarity_fn(s1_emb.unsqueeze(1), s2_emb.unsqueeze(0))
        # pairs to be separated
        contradiction_matrix = self.similarity_fn(s1_emb.unsqueeze(1), s3_emb.unsqueeze(0))
        # including contradiction sentences as negative samples for contrastive learning
        similarity_matrix = torch.cat([similarity_matrix, contradiction_matrix], dim=1) / self.hparams['cl_temp']

        # weighting if the nonentailment is a neutral or contradiction example
        # TODO: this part cannot be called in validation, it will affect the metrics
        weight_matrix = torch.diag(nli_weight)
        zeros = torch.zeros_like(contradiction_matrix)
        weight_matrix = torch.cat([zeros, weight_matrix], 1) * self.hparams['neg_weight']
        # contradiction_matrix = contradiction_matrix + weight_matrix

        similarity_matrix = similarity_matrix + weight_matrix
        return similarity_matrix

    def configure_optimizers(self):
        initial_lr = self.hparams['lr']
        optimizer = torch.optim.AdamW(params=self.parameters(), lr=initial_lr)
        # scheduler = ExponentialLR(optimizer, gamma=self.hparams['lr_decay'])

        t_max = self.hparams['training_epochs']
        scheduler = CosineAnnealingLR(optimizer, T_max=t_max)
        # scheduler = get_linear_schedule_with_warmup(optimizer, num_training_steps=self.hparams['training_epochs'], num_warmup_steps=0)
        # return {"optimizer": optimizer, "lr_scheduler": scheduler}

        return {"optimizer": optimizer}


# DEPRECATED CLASS
class PairwiseModel(L.LightningModule):

    def __init__(self,
                 hf_model_name: str,
                 training_epochs: int,
                 lr: float = 5e-5):
        super().__init__()
        self.save_hyperparameters("training_epochs", "hf_model_name", "lr")
        self.backbone_model = AutoModel.from_pretrained(hf_model_name, add_pooling_layer=False)
        self.similarity_fn = nn.CosineSimilarity(dim=-1)
        backbone_config = AutoConfig.from_pretrained(hf_model_name)
        self.mlp = nn.Sequential(
                nn.Linear(backbone_config.hidden_size, backbone_config.hidden_size),
                nn.Tanh()
        )

    def encode(self, x, include_mlp=True):  # TODO incluir mlp tambem em validation
        x = self.backbone_model(**x)
        x = x.last_hidden_state[:, 0]  # CLS token
        if include_mlp:
            x = self.mlp(x)
        return x

    def forward(self, batch):
        preprocess_batch = self._preprocess_batch(batch)
        s1_emb = self.encode(preprocess_batch['s1'])
        s2_emb = self.encode(preprocess_batch['s2'])
        e_sim = self.similarity_fn(s1_emb, s2_emb)
        # loss = (e_sim - batch['nli_objective']).abs().mean()
        loss = F.mse_loss(e_sim, batch['nli_objective'])
        return loss

    def training_step(self, batch, batch_idx):
        loss = self.forward(batch=batch)
        self.log("train_pairwise_loss", loss, prog_bar=True, on_step=True, on_epoch=True)
        return loss

    def _preprocess_batch(self, batch) -> Dict[str, Any]:
        """
        Retrieve each sentence BatchEncoder into a new batch
        :param batch: batch generated by dataset
        :return: dict containing all batches (s1, s2, s3, nli_label)
        """
        s1_batch = {k.replace('s1_', ''): v.to("cuda", non_blocking=True) for k, v in batch.items() if
                    k.startswith('s1')}
        s2_batch = {k.replace('s2_', ''): v.to("cuda", non_blocking=True) for k, v in batch.items() if
                    k.startswith('s2')}
        return {'s1': s1_batch,
                's2': s2_batch}

    def validation_step(self, batch, batch_idx):
        # loss = self._default_cl_validation(batch)

        loss = self.nli_validation(batch)
        return loss

    def nli_validation(self, batch):
        val_loss = self.forward(batch)
        self.log("val_nli_loss", val_loss, prog_bar=True, on_step=True, on_epoch=True)
        return val_loss

    def configure_optimizers(self):
        initial_lr = self.hparams['lr']
        optimizer = torch.optim.AdamW(params=self.parameters(), lr=initial_lr)
        # scheduler = ExponentialLR(optimizer, gamma=self.hparams['lr_decay'])

        t_max = self.hparams['training_epochs']
        scheduler = CosineAnnealingLR(optimizer, T_max=t_max)
        # scheduler = get_linear_schedule_with_warmup(optimizer, num_training_steps=self.hparams['training_epochs'], num_warmup_steps=0)
        return {"optimizer": optimizer, "lr_scheduler": scheduler}

        # return {"optimizer": optimizer}#
