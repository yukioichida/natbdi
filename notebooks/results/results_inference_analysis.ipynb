{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "pio.templates.default = \"simple_white\"\n",
    "\n",
    "from scienceworld import ScienceWorldEnv\n",
    "\n",
    "from sources.scienceworld import load_step_function, parse_observation\n",
    "from sources.agent import BDIAgent\n",
    "from sources.bdi_components.inference import NLIModel\n",
    "from sources.bdi_components.belief import State\n",
    "\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-09T08:50:08.293168Z",
     "end_time": "2023-10-09T08:50:08.400323Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "def preprocess_df(results_df):\n",
    "    results_df.loc[results_df[\"plans_pct\"] == 1, \"plans_pct\"] = 100\n",
    "    results_df.loc[results_df[\"plans_pct\"] == 2, \"plans_pct\"] = 25\n",
    "    results_df.loc[results_df[\"plans_pct\"] == 5, \"plans_pct\"] = 50\n",
    "    results_df.loc[results_df[\"plans_pct\"] == 7, \"plans_pct\"] = 75\n",
    "    results_df['rl_score'] = results_df['rl_score'] / 100\n",
    "    results_df['bdi_score'] = results_df['bdi_score'] / 100\n",
    "    results_df['final_score'] = results_df['final_score'] / 100\n",
    "    return results_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-09T08:50:08.363094Z",
     "end_time": "2023-10-09T08:50:08.400323Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-10-09T08:50:09.732372Z",
     "end_time": "2023-10-09T08:50:09.911360Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "plan_statistics = pd.read_csv(\"plan_statistics.csv\")\n",
    "\n",
    "dirs = [\"../results/v2-gchhablani-bert-base-cased-finetuned-mnli/\", \"../results/v2-MoritzLaurer-MiniLM-L6-mnli/\",\n",
    "        \"../results/v2-roberta-large-mnli/\"]  #, \"../results/v2-ynie-roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli/\"]\n",
    "#dirs = [\"../results/v2-minilm/\"]\n",
    "tasks = ['melt', 'find-non-living-thing']\n",
    "print(tasks)\n",
    "#files_overall = \"results_melt.csv\"\n",
    "#files_nli = \"results_nli_melt.csv\"\n",
    "\n",
    "all_overall_dfs = []\n",
    "all_nli_dfs = []\n",
    "for dir in dirs:\n",
    "    for task in tasks:\n",
    "        results_df = pd.read_csv(dir + f\"results_{task}.csv\")\n",
    "        results_df['task'] = task\n",
    "        all_overall_dfs.append(results_df)\n",
    "\n",
    "        nli_results_df = pd.read_csv(dir + f\"results_nli_{task}.csv\")\n",
    "        nli_results_df['task'] = task\n",
    "        all_nli_dfs.append(nli_results_df)\n",
    "\n",
    "overall_results_df = pd.concat(all_overall_dfs)\n",
    "overall_results_df = preprocess_df(overall_results_df)\n",
    "overall_results_df = pd.merge(overall_results_df, plan_statistics, on=['plans_pct', 'task'])\n",
    "overall_results_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "source": [
    "projected_cols = ['task', 'plans_pct', 'eps', 'num_specific_plans', 'nli_model']\n",
    "aggregations = {'variation': 'count', 'final_score': 'mean', 'rl_score': 'mean', 'bdi_score': 'mean',\n",
    "                'num_bdi_actions': 'mean', 'num_rl_actions': 'mean', 'error': 'mean', 'num_plans': 'mean'}\n",
    "\n",
    "grouped_df = overall_results_df.groupby(projected_cols).agg(aggregations).reset_index()\n",
    "grouped_df['dense_rank'] = (\n",
    "    grouped_df.groupby(['plans_pct', 'task', \"nli_model\"])['final_score'].rank(method='dense', ascending=False).astype(\n",
    "        int))\n",
    "\n",
    "#grouped_df = grouped_df.sort_values(['plans_pct', 'dense_rank'], ascending=[True, True]).reset_index()\n",
    "grouped_df = grouped_df[(grouped_df['dense_rank'] == 1)].sort_values([\"task\", \"num_specific_plans\", \"nli_model\"])\n",
    "# avoiding tied rows\n",
    "grouped_df.drop(columns=['dense_rank']).sort_values(by=['final_score', 'nli_model'], ascending=[False, False])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-09T08:50:10.508779Z",
     "end_time": "2023-10-09T08:50:10.572680Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "write_df = grouped_df.loc[:, ['task', 'nli_model', 'final_score', 'bdi_score', 'num_bdi_actions', 'error', 'num_plans']]\n",
    "write_df = write_df.replace('MoritzLaurer/MiniLM-L6-mnli', 'MiniLM L6')\n",
    "#write_df = write_df.replace('ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli', 'Roberta Large')\n",
    "write_df = write_df.replace('roberta-large-mnli', 'Roberta Large')\n",
    "write_df = write_df.replace('gchhablani/bert-base-cased-finetuned-mnli', 'Bert Base')\n",
    "write_df.rename(columns={\n",
    "    'task': 'Task',\n",
    "    'nli_model': 'Model',\n",
    "    'bdi_score': 'BDI Score',\n",
    "    'final_score': 'Score',\n",
    "    'error': 'Errors',\n",
    "    'num_plans': 'Num Plans',\n",
    "    'num_bdi_actions': 'Num Actions'\n",
    "}, inplace=True)\n",
    "\n",
    "write_df[['Task', 'Model', 'Score', 'BDI Score', 'Errors', 'Num Plans']].to_csv(\"nli_performance_results.csv\",\n",
    "                                                                                index=False, float_format='%.3f')\n",
    "write_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-09T08:50:11.304063Z",
     "end_time": "2023-10-09T08:50:11.333060Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "def lexical_overlap(a, b):\n",
    "    words_doc1 = set(a.split())\n",
    "    words_doc2 = set(b.split())\n",
    "\n",
    "    diff = words_doc1.intersection(words_doc2)\n",
    "    return len(diff)\n",
    "\n",
    "lexical_overlap(\"you see a pot\", \"you see a container\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-09T08:50:54.626895Z",
     "end_time": "2023-10-09T08:50:54.681253Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "source": [
    "nli_results_df = pd.concat(all_nli_dfs)\n",
    "print(len(nli_results_df))\n",
    "nli_results_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-09T08:50:58.329142Z",
     "end_time": "2023-10-09T08:50:58.362127Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "all_groups_df = []\n",
    "for model, group_df in nli_results_df.groupby(\"model\"):\n",
    "    print(model)\n",
    "    config = AutoConfig.from_pretrained(model)\n",
    "    #print(config.label2id)\n",
    "    #print(config.id2label)\n",
    "    group_df['inference'] = group_df['output'].apply(lambda id: config.id2label[id].lower())\n",
    "    all_groups_df.append(group_df)\n",
    "\n",
    "filtered_nli_df = pd.concat(all_groups_df)\n",
    "#filtered_nli_df['levenshtein_distance'] = filtered_nli_df.apply(lambda row: levenshtein_distance(row['p'], row['h']), axis=1)\n",
    "filtered_nli_df['lexical_overlap'] = filtered_nli_df.apply(lambda row: lexical_overlap(row['p'], row['h']), axis=1)\n",
    "filtered_nli_df['length_p'] = filtered_nli_df['p'].apply(lambda p: len(p.split()))\n",
    "filtered_nli_df['length_h'] = filtered_nli_df['h'].apply(lambda h: len(h.split()))\n",
    "\n",
    "filtered_nli_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-09T08:51:00.912270Z",
     "end_time": "2023-10-09T08:51:02.324309Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "source": [
    "all_statistics = []\n",
    "for (model, task), group_df in filtered_nli_df.groupby([\"model\", \"task\"]):\n",
    "    all_statistics.append({\n",
    "        'model': model,\n",
    "        'num_entailment': len(group_df[group_df['inference'] == 'entailment']),\n",
    "        #'num_neutral': len(group_df[group_df['inference'] == 'neutral']),\n",
    "        #'num_contradiction': len(group_df[group_df['inference'] == 'contradiction']),\n",
    "        'num_nonentailment': len(group_df[group_df['inference'] == 'neutral']) + len(\n",
    "            group_df[group_df['inference'] == 'contradiction']),\n",
    "        'num_inferences': len(group_df),\n",
    "        'mean_entailment_lexical_overlap': group_df[group_df['inference'] == 'entailment']['lexical_overlap'].mean(),\n",
    "        'mean_h': group_df[group_df['inference'] == 'entailment']['length_h'].mean(),\n",
    "        'mean_p': group_df[group_df['inference'] == 'entailment']['length_p'].mean(),\n",
    "        \"task\": task\n",
    "    })\n",
    "\n",
    "statistics_df = pd.DataFrame(all_statistics)\n",
    "#full_models_df = pd.merge(statistics_df, on='model', how='inner')\n",
    "statistics_df = statistics_df  #.drop(columns=['num_entailment', 'num_nonentailment'])\n",
    "statistics_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-09T08:51:04.065095Z",
     "end_time": "2023-10-09T08:51:04.096119Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "source": [
    "#write_df = statistics_df.loc[:, ['task', 'model', 'mean_entailment_lexical_overlap', 'mean_h', 'mean_p', 'num_entailment', 'num_nonentailment']]\n",
    "write_nli_df = statistics_df.loc[:,\n",
    "               ['task', 'model', 'mean_entailment_lexical_overlap', 'mean_h', 'mean_p', 'num_inferences']]\n",
    "write_nli_df = write_nli_df.replace('MoritzLaurer/MiniLM-L6-mnli', 'MiniLM L6')\n",
    "write_nli_df = write_nli_df.replace('roberta-large-mnli', 'Roberta Large')\n",
    "write_nli_df = write_nli_df.replace('gchhablani/bert-base-cased-finetuned-mnli', 'Bert Base')\n",
    "write_nli_df.rename(columns={\n",
    "    'task': 'Task',\n",
    "    'model': 'Model',\n",
    "    'mean_entailment_lexical_overlap': \"LO(E)\",\n",
    "    'mean_h': 'Mean Plan Context',\n",
    "    'mean_p': 'Mean Beliefs',\n",
    "    'num_inferences': 'Inferences'\n",
    "}, inplace=True)\n",
    "\n",
    "write_nli_df[['Task', 'Model', 'LO(E)', 'Mean Beliefs', 'Mean Plan Context', 'Inferences']].sort_values(\n",
    "    ['Task', 'Model']).to_csv(\"nli_inference_results.csv\", index=False, float_format='%.3f')\n",
    "write_nli_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-09T08:51:11.236329Z",
     "end_time": "2023-10-09T08:51:11.270585Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "source": [
    "0.64num_params = {\n",
    "    'Bert Base': 110,\n",
    "    'Roberta Large': 355,\n",
    "    'MiniLM L6': 22\n",
    "}\n",
    "\n",
    "# mnli - m\n",
    "mnli_results = {\n",
    "    'Bert Base': 84.6,\n",
    "    'Roberta Large': 90.8,\n",
    "    'MiniLM L6': 82.2\n",
    "}\n",
    "\n",
    "all_write_df = pd.merge(write_df, write_nli_df, on=['Model', 'Task'], how='inner')\n",
    "columns = ['Model', 'Params', 'MNLI-m', 'Task', 'Score', 'BDI Score', 'Num Actions', 'Errors',\n",
    "           'Num Plans', 'LO(E)', 'Mean Beliefs', 'Mean Plan Context',\n",
    "           'Inferences']\n",
    "all_write_df['Params'] = all_write_df['Model'].apply(lambda model: num_params[model])\n",
    "all_write_df['MNLI-m'] = all_write_df['Model'].apply(lambda model: mnli_results[model])\n",
    "all_write_df = all_write_df[columns].sort_values(['Params','Model', 'Task'])\n",
    "all_write_df.to_csv(\"nli_results.csv\", index=False, float_format='%.3f')\n",
    "all_write_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-08T10:58:04.326546Z",
     "end_time": "2023-10-08T10:58:04.344280Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "Não sei se faz sentido manter a qtd de entailment/non entailment, não tem muito o que falar disso. talvez falar alto nivel em future work\n",
    "\n",
    "We compute the lexical overlap to measure the inference difficult between the belief base and plan context that we manually developed in order to evaluate our approach.\n",
    "Specifically, given a sentence pair consisting in a belief and a context, we calculate the number of words contained in beliefs that are absent in plan context.\n",
    "In cases where lexical overlap is high between the premise and hypothesis, the inference tends to easily infer entailment relation since both sentences are similar and may express the same idea.\n",
    "Hence, in such cases, sophisticated language models exploit shallow syntactic heuristics to infer logical entailment between sentences. (citar paper HANS)\n",
    "We show that the number of lexical overlap is high when comparing to the average word number in both sentences.\n",
    "The average lexical overlap in entailment sentence pairs is higher than the average number of plan context words since most beliefs contain more words.\n",
    "\n",
    "The number of entailed pairs is significant low since the cartesian product between the belief base and plan contexts tends to generate very unrelated pairs.\n",
    "As future work, we plan to pruning very different sentence pairs in order to reduce the NLI model computation.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Não vale a pena medir ground thruth, pois minilm não gerou todos planos (falhou antes pois não deu sequência nos subgoals seguintes)\n",
    "\n",
    "# na real vale contar quantos falsos entailment e quantos falsos não entailment"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T08:21:39.838846Z",
     "end_time": "2023-10-06T08:21:39.879622Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for task, task_df in filtered_nli_df.groupby(\"task\"):\n",
    "    for model, model_df in task_df.groupby(\"model\"):\n",
    "        gt_df = model_df[['p', 'h', 'inference']]\n",
    "        gt_df['y'] = 1  # temp\n",
    "        gt_df.loc[gt_df['inference'] != 'entailment', 'inference'] = 'non_entailment'\n",
    "        gt_df['model'] = model\n",
    "        gt_df['task'] = task\n",
    "        gt_df.sort_values(['h', 'inference']).to_csv(f\"ground_truth_{task}_{model.replace('/', '-')}.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-07T22:24:18.745078Z",
     "end_time": "2023-10-07T22:24:18.833274Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
