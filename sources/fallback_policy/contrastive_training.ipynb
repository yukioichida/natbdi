{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-01T12:15:50.397014Z",
     "start_time": "2024-09-01T12:15:48.334781Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from sources.cl_nli.model import SimCSE\n",
    "from sources.fallback_policy.replay import ReplayMemory, Transition\n",
    "from sources.scienceworld import parse_beliefs"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Initializing q_network",
   "id": "423de3816c0643f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:15:52.409222Z",
     "start_time": "2024-09-01T12:15:50.398427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sources.fallback_policy.model import SimpleQNetwork, QNetwork\n",
    "\n",
    "use_transformer = True\n",
    "if use_transformer:\n",
    "    network = QNetwork(768, 768, n_blocks=4)\n",
    "    use_cls = True\n",
    "else:\n",
    "    network = SimpleQNetwork(768, 768, 1)\n",
    "    use_cls = False\n",
    "network = network.to('cuda')\n",
    "network.train()\n",
    "\n",
    "num_parameters = sum(p.numel() for p in network.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters: {num_parameters}\")\n",
    "network"
   ],
   "id": "3a7024541118e36c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 12983040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (belief_base_encoder): BeliefBaseEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0-3): 4 x BeliefTransformerBlock(\n",
       "        (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layer_norm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (qkv_proj_layer): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (mlp): PositionWiseFF(\n",
       "          (c_fc): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layer_norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (hidden): Linear(in_features=1536, out_features=768, bias=False)\n",
       "  (q_value_layer): Linear(in_features=768, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Reading goldpath trajectories",
   "id": "43b359c162afc26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:15:52.414155Z",
     "start_time": "2024-09-01T12:15:52.410308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "goldpath_file = \"/opt/data/scienceworld-goldpaths/goldsequences-0.json\"\n",
    "\n",
    "with open(goldpath_file) as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "json_data = json_data['0']\n",
    "\n",
    "json_data.keys()"
   ],
   "id": "665684dbb9595f29",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['taskIdx', 'taskName', 'goldActionSequences'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:15:54.462134Z",
     "start_time": "2024-09-01T12:15:52.415581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ckpt = \"/opt/models/simcse_default/version_0/v0-epoch=4-step=18304-val_nli_loss=0.658-train_loss=0.551.ckpt\"\n",
    "\n",
    "model: SimCSE = SimCSE.load_from_checkpoint(ckpt).eval()\n",
    "hf_model_name = model.hparams['hf_model_name']\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_name)"
   ],
   "id": "9e98b425dbabce41",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ichida/miniconda3/envs/default_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:15:55.700636Z",
     "start_time": "2024-09-01T12:15:54.463128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "simcse_tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-roberta-base\")\n",
    "simcse_model = AutoModel.from_pretrained(\"princeton-nlp/sup-simcse-roberta-base\").eval().to('cuda')"
   ],
   "id": "3465d8f449d64f6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create memory buffer",
   "id": "b8bd533771827901"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:15:55.706615Z",
     "start_time": "2024-09-01T12:15:55.701914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode_custom(texts: list[str], max_size: int = 25, include_cls: bool = True) -> torch.Tensor:\n",
    "    if include_cls:\n",
    "        cls_token = tokenizer.cls_token\n",
    "        texts = [cls_token] + texts\n",
    "    pad_size = max_size - len(texts)\n",
    "    padding = [tokenizer.pad_token for _ in range(pad_size)]\n",
    "    texts = texts + padding\n",
    "    tokenized_text = tokenizer(texts, padding='longest', truncation=True,\n",
    "                               return_tensors='pt').to(model.device)\n",
    "    embeddings = model.encode(tokenized_text).detach().unsqueeze(0)  # batch axis\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def encode_simcse(texts: list[str], max_size: int = 25, include_cls: bool = True) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        if include_cls:\n",
    "            cls_token = simcse_tokenizer.cls_token\n",
    "            texts = [cls_token] + texts\n",
    "        pad_size = max_size - len(texts)\n",
    "        padding = [simcse_tokenizer.pad_token for _ in range(pad_size)]\n",
    "        texts = texts + padding\n",
    "        tokenized_text = simcse_tokenizer(texts, padding='longest', truncation=True,\n",
    "                                          return_tensors='pt').to('cuda')\n",
    "        embeddings = simcse_model(**tokenized_text).pooler_output.unsqueeze(0)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "def encode(texts: list[str], max_size: int = 25, include_cls: bool = True) -> torch.Tensor:\n",
    "    return encode_simcse(texts, max_size, include_cls)\n",
    "    #return encode_custom(texts, max_size, include_cls)"
   ],
   "id": "7c004c5d2c3bf248",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:15:59.139040Z",
     "start_time": "2024-09-01T12:15:55.708142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gold_sequence = json_data['goldActionSequences'][0]['path']\n",
    "\n",
    "goal = json_data['goldActionSequences'][0]['taskDescription'].split('.')[0]\n",
    "variation_idx = json_data['goldActionSequences'][0]['variationIdx']\n",
    "print(f\"Goal: {goal} - variation {variation_idx}\")\n",
    "\n",
    "all_lens = []\n",
    "\n",
    "trajectories_bdi = []\n",
    "\n",
    "memory_buffer = ReplayMemory(1000)\n",
    "\n",
    "last_reward = 0\n",
    "observation = \"\"\n",
    "for i, trajectory in enumerate(gold_sequence):\n",
    "    look_around = trajectory['freelook']\n",
    "    inventory = trajectory['inventory']\n",
    "    belief_base = parse_beliefs(observation=observation, look=look_around, inventory=inventory)\n",
    "\n",
    "    next_trajectory = gold_sequence[i + 1]\n",
    "    next_belief_base = parse_beliefs(observation=next_trajectory['observation'],\n",
    "                                     look=next_trajectory['freelook'],\n",
    "                                     inventory=next_trajectory['inventory'])\n",
    "\n",
    "    reward = float(trajectory['score']) - last_reward\n",
    "    last_reward = float(trajectory['score'])\n",
    "    is_done = trajectory['isCompleted']\n",
    "    if is_done == 'true':\n",
    "        next_state = \"\"\n",
    "        print(\"finish\")\n",
    "        break\n",
    "        # ou break de repente aqui\n",
    "        \n",
    "    extra_beliefs = 2 if use_cls else 1\n",
    "    if trajectory['action'] != 'look around':\n",
    "        memory_buffer.push(\n",
    "                Transition(\n",
    "                        belief_base=encode(belief_base + [goal], include_cls=use_cls),\n",
    "                        num_beliefs=len(belief_base) + extra_beliefs,  # including goal\n",
    "                        action=encode([trajectory['action']], max_size=1, include_cls=False),\n",
    "                        next_belief_base=encode(next_belief_base + [goal]),\n",
    "                        num_next_beliefs=len(next_belief_base) + extra_beliefs,\n",
    "                        next_action=encode([next_trajectory['action']], max_size=1, include_cls=False),\n",
    "                        reward=reward,\n",
    "                        done=True if is_done == 'true' else False\n",
    "                )\n",
    "        )\n",
    "    observation = trajectory['observation']\n",
    "\n",
    "    #print(f\"Step {i} - reward: {reward:.3f} - is_done: {is_done}  - action: {trajectory['action']}\")"
   ],
   "id": "585ca7affeca7d46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal: Your task is to boil water - variation 0\n",
      "finish\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train q-network using annotated trajectories",
   "id": "c326011ec0529219"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initializing Network",
   "id": "17df7cca89af56d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:17:14.395322Z",
     "start_time": "2024-09-01T12:15:59.140058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# q-learning \n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "def contrastive_loss(belief_base_emb, num_belief_emb, action_emb):\n",
    "    # USING CONTRASTIVE LOSS  (cross entropy) does not work well\n",
    "    # maybe because we did not need to use a softmax function at the top of network to predict q-values\n",
    "    batch_size, _, _ = belief_base_emb.size()\n",
    "\n",
    "    belief_q_values = []\n",
    "    for belief_idx in range(BATCH_SIZE):\n",
    "        c_belief_base = belief_base_emb[belief_idx, :, :].unsqueeze(0)\n",
    "        c_belief_base = c_belief_base.repeat(batch_size, 1, 1)\n",
    "        q_values = network(belief_base=c_belief_base,\n",
    "                           belief_base_sizes=[num_belief_emb[belief_idx] for _ in range(batch_size)],\n",
    "                           action_tensors=action_emb)\n",
    "        belief_q_values.append(q_values.squeeze(0))\n",
    "\n",
    "    all_q_values = torch.cat(belief_q_values, dim=-1)\n",
    "    labels = torch.zeros_like(all_q_values).to('cuda')\n",
    "    labels = labels.fill_diagonal_(1)\n",
    "    all_q_values = all_q_values.view(batch_size * batch_size, 1)\n",
    "    labels = labels.view(batch_size * batch_size, 1)\n",
    "    #return F.mse_loss(all_q_values, labels)\n",
    "    return F.smooth_l1_loss(all_q_values, labels)\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "    batch = memory_buffer.sample(BATCH_SIZE)\n",
    "    belief_base_emb = torch.cat([b.belief_base for b in batch], dim=0)\n",
    "    batch_size, _, belief_dim = belief_base_emb.size()\n",
    "    num_belief_emb = [b.num_beliefs for b in batch]\n",
    "    actions = torch.cat([b.action for b in batch]).squeeze(1)  # removing mid axis [bs, ?, a_dim] -> [bs, a_dim]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = contrastive_loss(belief_base_emb, num_belief_emb, actions)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"epoch {epoch} - loss {loss.item(): .4f}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #break\n",
    "    #nn.utils.clip_grad_norm_(network.parameters(), 1.)\n",
    "\n",
    "print(f\"epoch {epoch} - loss {loss.item(): .4f}\")\n",
    "# simple network epoch 299 - loss  0.0219"
   ],
   "id": "76073dcf3fff52a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 - loss  0.0738\n",
      "epoch 100 - loss  0.0544\n",
      "epoch 200 - loss  0.0529\n",
      "epoch 300 - loss  0.0488\n",
      "epoch 400 - loss  0.0421\n",
      "epoch 500 - loss  0.0497\n",
      "epoch 600 - loss  0.0444\n",
      "epoch 700 - loss  0.0420\n",
      "epoch 800 - loss  0.0430\n",
      "epoch 900 - loss  0.0360\n",
      "epoch 999 - loss  0.0265\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluating trajectory samples",
   "id": "d55a16fd92a64c41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:17:14.400626Z",
     "start_time": "2024-09-01T12:17:14.396634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "network = network.eval()\n",
    "turn = 2\n",
    "expected_action = gold_sequence[turn]['action']\n",
    "#print(expected_action)\n",
    "annotated_belief_base = parse_beliefs(observation=gold_sequence[turn - 1]['observation'],\n",
    "                                      look=gold_sequence[turn]['freelook'],\n",
    "                                      inventory=gold_sequence[turn]['inventory']) + [goal]#['your task is to melt gallium']\n",
    "\n",
    "annotated_belief_base"
   ],
   "id": "4d8219195d3e7a1d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This room is called the hallway.',\n",
       " 'You see a picture',\n",
       " 'You see a substance called air',\n",
       " 'You see the agent',\n",
       " 'A door to the green house (that is open)',\n",
       " 'A door to the living room (that is open)',\n",
       " 'A door to the art studio (that is open)',\n",
       " 'A door to the kitchen (that is open)',\n",
       " 'A door to the bedroom (that is open)',\n",
       " 'A door to the workshop (that is open)',\n",
       " 'In your inventory, you see: an orange ',\n",
       " 'The door is already open.',\n",
       " 'Your task is to boil water']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:17:14.462290Z",
     "start_time": "2024-09-01T12:17:14.402776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "extra_beliefs = 2 if use_cls else 1\n",
    "num_beliefs = len(annotated_belief_base) + extra_beliefs\n",
    "\n",
    "encoded_belief_base = encode(annotated_belief_base, max_size=len(annotated_belief_base), include_cls=use_cls)\n",
    "\n",
    "#candidate_actions = info['valid']\n",
    "candidate_actions = ['focus on bedroom door', 'open door to kitchen', 'go to kitchen', 'look at the kitchen', 'go to door to the kitchen'] + [\n",
    "        expected_action]\n",
    "encoded_actions = encode(candidate_actions, include_cls=False, max_size=len(candidate_actions))\n",
    "encoded_actions = encoded_actions.squeeze(0)\n",
    "num_actions, action_dim = encoded_actions.size()\n",
    "repeat_encoded_belief_base = encoded_belief_base.repeat(num_actions, 1, 1)\n",
    "q_values = network(belief_base=repeat_encoded_belief_base,\n",
    "                   belief_base_sizes=[len(annotated_belief_base) + 1 for _ in range(num_actions)],\n",
    "                   action_tensors=encoded_actions)\n",
    "\n",
    "print(expected_action)\n",
    "values, idxs = torch.sort(q_values.squeeze(1), descending=True)\n",
    "print(\"Action space - Top 5:\")\n",
    "for i, idx in enumerate(idxs[:5]):\n",
    "    print(f\"\\tCandidate Action: {candidate_actions[idx]} - q_value: {values[i]:.3f}\")\n",
    "    \n",
    "selected_idx = idxs[0]\n",
    "print(f\"Predicted action: {candidate_actions[selected_idx]}\")"
   ],
   "id": "ce878dffffd298de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go to kitchen\n",
      "Action space - Top 5:\n",
      "\tCandidate Action: go to door to the kitchen - q_value: 0.735\n",
      "\tCandidate Action: open door to kitchen - q_value: 0.714\n",
      "\tCandidate Action: go to kitchen - q_value: 0.675\n",
      "\tCandidate Action: go to kitchen - q_value: 0.675\n",
      "\tCandidate Action: look at the kitchen - q_value: 0.639\n",
      "Predicted action: go to door to the kitchen\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:17:14.465992Z",
     "start_time": "2024-09-01T12:17:14.463240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scienceworld import ScienceWorldEnv\n",
    "\n",
    "network = network.eval()"
   ],
   "id": "2d431e34e4857dc1",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:17:15.122573Z",
     "start_time": "2024-09-01T12:17:14.467060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = ScienceWorldEnv()\n",
    "variation_idx = json_data['goldActionSequences'][0]['variationIdx']\n"
   ],
   "id": "7f44284b7ca9a03",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:17:15.574706Z",
     "start_time": "2024-09-01T12:17:15.123652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env.load(\"boil\", variation_idx, \"openDoors\")\n",
    "with torch.no_grad():\n",
    "    max_steps = 1\n",
    "    action = \"look around\"\n",
    "    \n",
    "    blacklist_action = []\n",
    "    for i in range(max_steps):\n",
    "        obs, reward, is_done, info = env.step(action)\n",
    "        \n",
    "        # if obs is non action that matches the input then\n",
    "        #    remove action from info['valid']\n",
    "        # else\n",
    "        #    do nothing\n",
    "        \n",
    "        print(f\"Step {i} - reward: {reward:.3f} - is_done: {is_done} - action: {action}\")\n",
    "        belief_base = parse_beliefs(observation=obs, look=info['look'], inventory=info['inv']) + [goal]\n",
    "        belief_base = [b.replace(\"greenhouse\", \"green house\") for b in belief_base]\n",
    "        num_beliefs = len(belief_base) + extra_beliefs\n",
    "        encoded_belief_base = encode(belief_base, max_size=len(belief_base))\n",
    "        encoded_actions = encode(info['valid'], max_size=len(info['valid']), include_cls=False)\n",
    "        encoded_actions = encoded_actions.squeeze(0)\n",
    "        num_actions, action_dim = encoded_actions.size()\n",
    "        repeat_encoded_belief_base = encoded_belief_base.repeat(num_actions, 1, 1)\n",
    "        q_values = network(belief_base=repeat_encoded_belief_base,\n",
    "                           belief_base_sizes=[num_beliefs],\n",
    "                           action_tensors=encoded_actions)\n",
    "        selected_action = q_values.argmax()\n",
    "        action = info['valid'][selected_action.item()]\n"
   ],
   "id": "e6a300a69dd4becb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 - reward: 0.000 - is_done: False - action: look around\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:17:15.590252Z",
     "start_time": "2024-09-01T12:17:15.575882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "values, idxs = torch.sort(q_values.squeeze(1), descending=True)\n",
    "top_k = 10\n",
    "print(f\"Action space - Top {top_k}:\")\n",
    "for i, idx in enumerate(idxs[:top_k]):\n",
    "    print(f\"\\tCandidate Action: {info['valid'][idx]} - q_value: {values[i]:.3f}\")\n"
   ],
   "id": "b4d253ca20516ce6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space - Top 10:\n",
      "\tCandidate Action: go to door to kitchen - q_value: 0.718\n",
      "\tCandidate Action: go to kitchen - q_value: 0.674\n",
      "\tCandidate Action: look at door to kitchen - q_value: 0.671\n",
      "\tCandidate Action: connect hallway to kitchen - q_value: 0.642\n",
      "\tCandidate Action: mix door to kitchen - q_value: 0.621\n",
      "\tCandidate Action: connect bedroom door to kitchen - q_value: 0.619\n",
      "\tCandidate Action: focus on door to kitchen - q_value: 0.598\n",
      "\tCandidate Action: connect kitchen to hallway - q_value: 0.598\n",
      "\tCandidate Action: connect hallway to door to kitchen - q_value: 0.587\n",
      "\tCandidate Action: look at kitchen - q_value: 0.584\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:17:15.594371Z",
     "start_time": "2024-09-01T12:17:15.591219Z"
    }
   },
   "cell_type": "code",
   "source": "info['valid'].index(\"go to kitchen\")",
   "id": "3f940d8e09ad40d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:17:15.599673Z",
     "start_time": "2024-09-01T12:17:15.595423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(belief_base) + extra_beliefs)\n",
    "belief_base"
   ],
   "id": "41506f6c43181526",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This room is called the hallway.',\n",
       " 'You see the agent',\n",
       " 'You see a substance called air',\n",
       " 'You see a picture',\n",
       " 'A door to the art studio (that is open)',\n",
       " 'A door to the bedroom (that is open)',\n",
       " 'A door to the green house (that is open)',\n",
       " 'A door to the kitchen (that is open)',\n",
       " 'A door to the living room (that is open)',\n",
       " 'A door to the workshop (that is open)',\n",
       " 'In your inventory, you see: an orange ',\n",
       " 'Your task is to boil water']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:17:15.603728Z",
     "start_time": "2024-09-01T12:17:15.600557Z"
    }
   },
   "cell_type": "code",
   "source": "[b for b in belief_base if b not in annotated_belief_base]",
   "id": "6904ceb385ced7a2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:17:15.608198Z",
     "start_time": "2024-09-01T12:17:15.605009Z"
    }
   },
   "cell_type": "code",
   "source": "[b for b in annotated_belief_base if b not in belief_base]",
   "id": "389496289c1393d6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The door is already open.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generating Plans",
   "id": "cb374160fe891cc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:17:16.272351Z",
     "start_time": "2024-09-01T12:17:15.609272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_nouns(text:str) -> list[str]:\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc if token.pos_ in (\"NOUN\", \"PROPN\")]\n",
    "\n",
    "action_nouns = get_nouns(expected_action)\n",
    "\n",
    "plan_context = []\n",
    "for belief in annotated_belief_base:\n",
    "    nouns = get_nouns(belief)\n",
    "    if set(action_nouns) <= set(nouns):\n",
    "        print(f\"\\nGoal: {goal}\\nContext: {belief}. \\nAction: {expected_action}\")\n",
    "        plan_context.append(belief)\n",
    "    \n",
    "plan = f\"\"\"\n",
    "IF {goal}\n",
    "CONSIDERING {'AND'.join(plan_context)}\n",
    "THEN\n",
    "{expected_action}\n",
    "\"\"\"\n",
    "\n",
    "print(plan)\n",
    "    "
   ],
   "id": "df58742b74adcac6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Goal: Your task is to boil water\n",
      "Context: A door to the kitchen (that is open). \n",
      "Action: go to kitchen\n",
      "\n",
      "IF Your task is to boil water\n",
      "CONSIDERING A door to the kitchen (that is open)\n",
      "THEN\n",
      "go to kitchen\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:17:16.327624Z",
     "start_time": "2024-09-01T12:17:16.273316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "candidate_context = [\n",
    "        \"you see a cupboard\",\n",
    "        \"you don't see a cupboard\",\n",
    "        \"you are not in the kitchen\",\n",
    "        \"you are in the kitchen\"\n",
    "]\n",
    "\n",
    "action = \"you see a cupboard\"\n",
    "\n",
    "THRESHOLD = 0.7\n",
    "\n",
    "for i, candidate in enumerate(candidate_context):\n",
    "    embeddings = encode_simcse([candidate_context[i], action], max_size=2, include_cls=False)\n",
    "    embeddings_a = embeddings[:, 0, :]\n",
    "    embeddings_b = embeddings[:, 1, :]\n",
    "    \n",
    "    sim = nn.functional.cosine_similarity(embeddings_a, embeddings_b)\n",
    "    print(f\"Candidate context: {candidate} - Similarity: {sim.item():.4f} - [{sim.item() >= THRESHOLD}]\")\n",
    "    "
   ],
   "id": "786825212d27361c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate context: you see a cupboard - Similarity: 1.0000 - [True]\n",
      "Candidate context: you don't see a cupboard - Similarity: 0.5476 - [False]\n",
      "Candidate context: you are not in the kitchen - Similarity: 0.3883 - [False]\n",
      "Candidate context: you are in the kitchen - Similarity: 0.5414 - [False]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:18:01.915524Z",
     "start_time": "2024-09-01T12:18:01.858011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "candidate_context = [\n",
    "        \"look at the kitchen\",\n",
    "        \"open door to the kitchen\",\n",
    "        \"go to kitchen\",\n",
    "        \"connect hallway to kitchen\"\n",
    "]\n",
    "\n",
    "action = \"go to kitchen\"\n",
    "\n",
    "THRESHOLD = 0.7\n",
    "\n",
    "for i, candidate in enumerate(candidate_context):\n",
    "    embeddings = encode_simcse([candidate_context[i], action], max_size=2, include_cls=False)\n",
    "    embeddings_a = embeddings[:, 0, :]\n",
    "    embeddings_b = embeddings[:, 1, :]\n",
    "    \n",
    "    sim = nn.functional.cosine_similarity(embeddings_a, embeddings_b)\n",
    "    print(f\"Candidate context: {candidate} - Similarity: {sim.item():.4f} - [{sim.item() >= THRESHOLD}]\")\n",
    "    "
   ],
   "id": "b75eec446add4091",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate context: look at the kitchen - Similarity: 0.7573 - [True]\n",
      "Candidate context: open door to the kitchen - Similarity: 0.7459 - [True]\n",
      "Candidate context: go to kitchen - Similarity: 1.0000 - [True]\n",
      "Candidate context: connect hallway to kitchen - Similarity: 0.7748 - [True]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T12:17:16.385208Z",
     "start_time": "2024-09-01T12:17:16.381787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Candidate context: look at the kitchen - Similarity: 0.9610 - [True]\n",
    "Candidate context: open door to the kitchen - Similarity: 0.5093 - [False]\n",
    "Candidate context: go to the kitchen - Similarity: 1.0000 - [True]\n",
    "Candidate context: connect hallway to kitchen - Similarity: 0.9258 - [True]"
   ],
   "id": "47ef7ad5cce7fef0",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4119736769.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[22], line 1\u001B[0;36m\u001B[0m\n\u001B[0;31m    Candidate context: look at the kitchen - Similarity: 0.9610 - [True]\u001B[0m\n\u001B[0m              ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
