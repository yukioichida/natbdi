{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-16T10:53:13.055106Z",
     "start_time": "2024-08-16T10:53:12.177480Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sources.fallback_policy.model import QNetwork"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:53:14.304807Z",
     "start_time": "2024-08-16T10:53:13.056730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# belief_base -- a1 --> belief_base a2\n",
    "# belief_base -- a2 --> belief_base a3\n",
    "\n",
    "\n",
    "embedding_dim = 768\n",
    "num_belief = 2 + 1 # including cls\n",
    "\n",
    "num_actions = 2\n",
    "num_future_actions = 1\n",
    "\n",
    "cls_belief = torch.randn(1, embedding_dim).unsqueeze(0)\n",
    "\n",
    "belief_base = torch.randn(num_belief, embedding_dim).unsqueeze(0)\n",
    "belief_base = torch.cat([cls_belief, belief_base], dim=1)\n",
    "\n",
    "next_belief_base_a = torch.randn(num_belief, embedding_dim).unsqueeze(0)\n",
    "next_belief_base_a = torch.cat([cls_belief, next_belief_base_a], dim=1)\n",
    "\n",
    "next_belief_base_b = torch.randn(num_belief, embedding_dim).unsqueeze(0)\n",
    "next_belief_base_b = torch.cat([cls_belief, next_belief_base_b], dim=1)\n",
    "\n",
    "\n",
    "goal = torch.randn(1, embedding_dim)\n",
    "\n",
    "action_a = torch.randn(1, embedding_dim)\n",
    "action_b = torch.randn(1, embedding_dim)\n",
    "action_c = torch.randn(1, embedding_dim)\n",
    "action_d = torch.randn(1, embedding_dim)\n",
    "\n",
    "transition_a = (belief_base, action_a, next_belief_base_a, action_c, 0.7, goal)\n",
    "transition_b = (belief_base, action_b, next_belief_base_b, action_d, 0, goal)\n",
    "\n",
    "batch_belief_base = torch.cat([transition_a[0], transition_b[0]]).to('cuda')\n",
    "batch_action = torch.cat([transition_a[1], transition_b[1]]).to('cuda')\n",
    "batch_next_belief_base = torch.cat([transition_a[2], transition_b[2]]).to('cuda')\n",
    "batch_next_actions = torch.cat([transition_a[3], transition_b[3]]).to('cuda')\n",
    "batch_reward = torch.tensor([transition_a[4], transition_b[4]], dtype=torch.float).to('cuda')\n",
    "batch_goal = torch.cat([transition_a[5], transition_b[5]]).to('cuda')\n",
    "batch_reward"
   ],
   "id": "11f2c57cc44f37f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7000, 0.0000], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:53:14.331408Z",
     "start_time": "2024-08-16T10:53:14.305932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "network = QNetwork(embedding_dim, embedding_dim, embedding_dim, n_blocks=1)\n",
    "network = network.to('cuda')\n",
    "network"
   ],
   "id": "b58f23932a3d148a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (belief_base_encoder): BeliefBaseEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): BeliefTransformerBlock(\n",
       "        (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layer_norm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (qkv_proj_layer): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (mlp): PositionWiseFF(\n",
       "          (c_fc): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layer_norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (hidden): Linear(in_features=1536, out_features=768, bias=False)\n",
       "  (q_value_layer): Linear(in_features=768, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "q(belief_base, a)",
   "id": "e37215bdd51a3491"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:53:15.830185Z",
     "start_time": "2024-08-16T10:53:14.332405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# q-learning \n",
    "GAMMA = 0.99\n",
    "\n",
    "num_parameters = sum(p.numel() for p in network.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters: {num_parameters}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    # Q(s', a')\n",
    "    next_q_values = network(belief_base=batch_next_belief_base, \n",
    "                            belief_base_sizes=[num_belief], \n",
    "                            action_tensors=batch_next_actions)\n",
    "    best_next_q_values, _ = next_q_values.max(dim=0)\n",
    "    targets = batch_reward  + (GAMMA * best_next_q_values)\n",
    "    \n",
    "    # Q(s, a)\n",
    "    q_values = network(belief_base=batch_belief_base, belief_base_sizes=[num_belief], action_tensors=batch_action)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"[{epoch}] q-values {q_values.squeeze(-1)}, targets {targets.squeeze(-1)}\")\n",
    "    loss = F.smooth_l1_loss(q_values.squeeze(-1), targets.detach())\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(network.parameters(), 5.)\n",
    "    optimizer.step()\n",
    "    #break"
   ],
   "id": "b8eabda64b491d76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 4131072\n",
      "[0] q-values tensor([-0.1408, -0.0129], device='cuda:0', grad_fn=<SqueezeBackward1>), targets tensor([0.9366, 0.2366], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "[10] q-values tensor([0.9142, 0.2249], device='cuda:0', grad_fn=<SqueezeBackward1>), targets tensor([0.9116, 0.2116], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "[20] q-values tensor([0.9517, 0.1589], device='cuda:0', grad_fn=<SqueezeBackward1>), targets tensor([0.8993, 0.1993], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "[30] q-values tensor([0.9421, 0.1588], device='cuda:0', grad_fn=<SqueezeBackward1>), targets tensor([0.8957, 0.1957], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "[40] q-values tensor([0.9024, 0.1880], device='cuda:0', grad_fn=<SqueezeBackward1>), targets tensor([0.8950, 0.1950], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "[50] q-values tensor([0.8773, 0.2068], device='cuda:0', grad_fn=<SqueezeBackward1>), targets tensor([0.8949, 0.1949], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "[60] q-values tensor([0.8971, 0.1924], device='cuda:0', grad_fn=<SqueezeBackward1>), targets tensor([0.8943, 0.1943], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "[70] q-values tensor([0.8974, 0.1923], device='cuda:0', grad_fn=<SqueezeBackward1>), targets tensor([0.8943, 0.1943], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "[80] q-values tensor([0.8913, 0.1964], device='cuda:0', grad_fn=<SqueezeBackward1>), targets tensor([0.8944, 0.1944], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "[90] q-values tensor([0.8962, 0.1929], device='cuda:0', grad_fn=<SqueezeBackward1>), targets tensor([0.8943, 0.1943], device='cuda:0', grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:53:15.836458Z",
     "start_time": "2024-08-16T10:53:15.832026Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.memory_allocated('cuda') / 1024.0 / 1024.0",
   "id": "e4d204c60e97bc0e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.56103515625"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T10:53:15.840213Z",
     "start_time": "2024-08-16T10:53:15.837957Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c302524c62ee5174",
   "outputs": [],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
